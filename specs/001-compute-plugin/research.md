# Research: Compute Plugin ABC with Multi-Compute Pipeline Support

**Date**: 2026-01-09
**Feature**: 001-compute-plugin

## Research Summary

This document consolidates research findings for implementing the ComputePlugin ABC and DuckDB reference implementation. All NEEDS CLARIFICATION items from initial planning have been resolved.

**Key Architectural Decision**: Hybrid approach - ComputePlugin generates dbt profiles.yml (dbt handles ALL SQL execution via its adapters) AND provides lightweight `validate_connection()` using native database drivers for fast health checks.

---

## 1. dbt-duckdb Profile Configuration

### Decision
Generate profiles.yml with DuckDB-specific settings supporting in-memory, file-based, and Iceberg-attached modes.

### Rationale
- dbt-duckdb adapter v1.9+ uses type: `duckdb` with `path` for database location
- Extensions (iceberg, httpfs) are auto-loaded when attach blocks reference Iceberg tables
- Memory limits controlled via `settings.memory_limit` in profile
- Thread count controls parallelism for query execution

### Profile Structure
```yaml
# profiles.yml structure for dbt-duckdb
floe_project:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: ':memory:'  # or /path/to/database.duckdb
      threads: 4
      extensions:
        - iceberg
        - httpfs
      settings:
        memory_limit: '4GB'
        threads: 4
      # Secrets for catalog authentication
      secrets:
        - type: s3
          region: us-east-1
          key_id: "{{ env_var('AWS_ACCESS_KEY_ID') }}"
          secret: "{{ env_var('AWS_SECRET_ACCESS_KEY') }}"
      # Iceberg catalog attachment
      attach:
        - path: "iceberg:catalog_name"
          alias: iceberg_catalog
          type: iceberg
          options:
            catalog_uri: "http://polaris:8181/api/catalog"
            catalog_name: floe
            oauth_server_uri: "http://polaris:8181/api/catalog/v1/oauth/tokens"
            credential: "{{ env_var('POLARIS_CLIENT_ID') }}:{{ env_var('POLARIS_CLIENT_SECRET') }}"
```

### Key Configuration Options
| Option | Description | Default |
|--------|-------------|---------|
| `path` | Database path (`:memory:` for in-memory) | `:memory:` |
| `threads` | Parallel query threads | 4 |
| `extensions` | DuckDB extensions to load | `[]` |
| `settings.memory_limit` | Max memory for DuckDB | System default |
| `attach` | External catalogs to attach | `[]` |
| `secrets` | Authentication credentials | `[]` |

### Alternatives Considered
| Alternative | Rejected Because |
|------------|------------------|
| External duckdb_settings.sql | Less portable, not dbt-native |
| Runtime configuration | Settings should be declarative in profiles.yml |
| Hardcoded paths | Requires environment-specific configuration |

---

## 2. DuckDB Iceberg Extension Integration

### Decision
Use `ATTACH` statements with `iceberg_scan()` and REST catalog OAuth2 authentication for Polaris integration.

### Rationale
- DuckDB Iceberg extension supports REST catalogs via `ATTACH ... (TYPE ICEBERG)`
- OAuth2 client credentials flow for Polaris authentication
- Catalog attachment runs at connection time, not query time
- SQL statements are generated by `get_catalog_attachment_sql()` method

### Catalog Attachment SQL Pattern
```sql
-- Install and load extensions (dbt-duckdb handles this automatically)
INSTALL iceberg;
LOAD iceberg;
INSTALL httpfs;
LOAD httpfs;

-- Create secret for OAuth2 authentication
CREATE OR REPLACE SECRET polaris_oauth (
    TYPE S3,
    KEY_ID '${POLARIS_CLIENT_ID}',
    SECRET '${POLARIS_CLIENT_SECRET}',
    REGION 'us-east-1'
);

-- Attach Iceberg catalog via REST
ATTACH 'iceberg:polaris_catalog' AS iceberg_catalog (
    TYPE ICEBERG,
    CATALOG_URI 'http://polaris:8181/api/catalog',
    CATALOG_NAME 'floe',
    OAUTH_SERVER_URI 'http://polaris:8181/api/catalog/v1/oauth/tokens',
    CREDENTIAL '${POLARIS_CLIENT_ID}:${POLARIS_CLIENT_SECRET}'
);
```

### Query Pattern (After Attachment)
```sql
-- Read from Iceberg table via attached catalog
SELECT * FROM iceberg_catalog.my_namespace.my_table;

-- Or use iceberg_scan directly
SELECT * FROM iceberg_scan('s3://bucket/path/to/table');
```

### Authentication Methods
| Method | Use Case | Configuration |
|--------|----------|---------------|
| OAuth2 Client Credentials | Polaris REST catalog | `CREDENTIAL 'client_id:client_secret'` |
| AWS IAM | S3-backed catalogs | `CREATE SECRET` with AWS keys |
| Bearer Token | Pre-authenticated | `BEARER_TOKEN 'token'` |

### Alternatives Considered
| Alternative | Rejected Because |
|------------|------------------|
| Direct S3 path scanning | No catalog governance, no schema evolution |
| PyIceberg in Python | Violates "dbt owns SQL" principle |
| Hive metastore | Polaris REST catalog is target architecture |

---

## 3. K8s Resource Specifications

### Decision
Use Pydantic model `ResourceSpec` with workload-size presets (small, medium, large) for dbt job pod sizing.

### Rationale
- dbt jobs run as K8s Jobs, need CPU/memory requests and limits
- Presets simplify configuration for data engineers
- Custom overrides possible via direct ResourceSpec instantiation
- Ephemeral storage needed for dbt manifest/target directory

### ResourceSpec Model
```python
from pydantic import BaseModel, Field

class ResourceSpec(BaseModel):
    """K8s resource requirements for dbt job pods."""

    cpu_request: str = Field(default="100m", description="CPU request (K8s format)")
    cpu_limit: str = Field(default="1000m", description="CPU limit (K8s format)")
    memory_request: str = Field(default="256Mi", description="Memory request (K8s format)")
    memory_limit: str = Field(default="1Gi", description="Memory limit (K8s format)")
    ephemeral_storage_request: str = Field(default="100Mi", description="Ephemeral storage request")
    ephemeral_storage_limit: str = Field(default="1Gi", description="Ephemeral storage limit")
```

### Workload Size Presets
| Size | CPU Request | CPU Limit | Memory Request | Memory Limit | Use Case |
|------|-------------|-----------|----------------|--------------|----------|
| `small` | 100m | 500m | 256Mi | 512Mi | Simple transforms, dev |
| `medium` | 500m | 2000m | 1Gi | 4Gi | Standard production |
| `large` | 2000m | 8000m | 4Gi | 16Gi | Heavy aggregations |

### Implementation Pattern
```python
WORKLOAD_PRESETS: dict[str, ResourceSpec] = {
    "small": ResourceSpec(
        cpu_request="100m", cpu_limit="500m",
        memory_request="256Mi", memory_limit="512Mi",
        ephemeral_storage_request="100Mi", ephemeral_storage_limit="500Mi"
    ),
    "medium": ResourceSpec(
        cpu_request="500m", cpu_limit="2000m",
        memory_request="1Gi", memory_limit="4Gi",
        ephemeral_storage_request="500Mi", ephemeral_storage_limit="2Gi"
    ),
    "large": ResourceSpec(
        cpu_request="2000m", cpu_limit="8000m",
        memory_request="4Gi", memory_limit="16Gi",
        ephemeral_storage_request="1Gi", ephemeral_storage_limit="5Gi"
    ),
}

def get_resource_requirements(workload_size: str) -> ResourceSpec:
    """Return K8s resource requirements for dbt job pods."""
    if workload_size not in WORKLOAD_PRESETS:
        raise ValueError(f"Unknown workload size: {workload_size}. Use: small, medium, large")
    return WORKLOAD_PRESETS[workload_size]
```

### Alternatives Considered
| Alternative | Rejected Because |
|------------|------------------|
| Auto-scaling based on query | Too complex, unpredictable costs |
| No presets (always custom) | Poor UX for data engineers |
| Unlimited resources | K8s best practice requires limits |

---

## 4. Connection Validation Strategy

### Decision
Use native database drivers for lightweight health checks, not dbt debug.

### Rationale
- `dbt debug` is slow (starts full dbt runtime)
- Native drivers (duckdb, snowflake-connector-python) are faster
- Health check should complete in <5s per SC-007
- Returns structured `ConnectionResult` with latency metrics

### Implementation Pattern
```python
from dataclasses import dataclass
from enum import Enum
import time

class ConnectionStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

@dataclass
class ConnectionResult:
    """Result of validate_connection() health check."""
    status: ConnectionStatus
    latency_ms: float
    message: str = ""
    warnings: list[str] = field(default_factory=list)

def validate_connection(self, config: ComputeConfig) -> ConnectionResult:
    """Test connection using native DuckDB driver."""
    start = time.perf_counter()
    try:
        import duckdb
        conn = duckdb.connect(config.path)
        conn.execute("SELECT 1")
        conn.close()
        latency = (time.perf_counter() - start) * 1000
        return ConnectionResult(
            status=ConnectionStatus.HEALTHY,
            latency_ms=latency,
            message="Connection successful"
        )
    except Exception as e:
        latency = (time.perf_counter() - start) * 1000
        return ConnectionResult(
            status=ConnectionStatus.UNHEALTHY,
            latency_ms=latency,
            message=str(e)
        )
```

### Validation Checks
| Check | Purpose | Failure Mode |
|-------|---------|--------------|
| Basic connect | Database reachable | UNHEALTHY |
| Simple query (`SELECT 1`) | Query engine working | UNHEALTHY |
| Extension load | Iceberg extension available | DEGRADED |
| Catalog attach | REST catalog reachable | DEGRADED |

### Alternatives Considered
| Alternative | Rejected Because |
|------------|------------------|
| `dbt debug` | Too slow (>10s), requires full dbt context |
| No validation | Can't detect issues proactively |
| Full query test | Overkill for health check |

---

## 5. dbt Package Requirements

### Decision
`get_required_dbt_packages()` returns list of dbt adapter packages needed for pip install.

### Rationale
- Each compute target requires specific dbt adapter
- Package names follow `dbt-{adapter}` convention
- Version constraints ensure compatibility
- Used by deployment to build dbt job container

### Package Mapping
| Compute | Required Packages |
|---------|------------------|
| DuckDB | `["dbt-duckdb>=1.9.0"]` |
| Snowflake | `["dbt-snowflake>=1.8.0"]` |
| Spark | `["dbt-spark>=1.8.0", "PyHive>=0.7.0"]` |
| BigQuery | `["dbt-bigquery>=1.8.0"]` |

### Implementation Pattern
```python
class DuckDBComputePlugin(ComputePlugin):
    def get_required_dbt_packages(self) -> list[str]:
        """Return dbt packages required for this compute target."""
        return [
            "dbt-duckdb>=1.9.0",
            "duckdb>=0.9.0",  # Native driver for validate_connection()
        ]
```

---

## 6. ComputeConfig Model Design

### Decision
Single Pydantic model containing plugin name, connection settings, and compute-specific options.

### Rationale
- Passed to `generate_dbt_profile()` and `validate_connection()`
- Contains all information needed to connect to compute target
- Plugin-specific fields via `extra="allow"` or discriminated union

### Model Structure
```python
from pydantic import BaseModel, Field, SecretStr

class ComputeConfig(BaseModel):
    """Configuration for a compute target."""

    plugin: str = Field(..., description="Compute plugin name (e.g., 'duckdb')")

    # Common settings
    timeout_seconds: int = Field(default=3600, ge=1, description="Query timeout")
    threads: int = Field(default=4, ge=1, le=64, description="Parallel threads")

    # Connection settings (plugin-specific)
    connection: dict[str, Any] = Field(default_factory=dict)

    # Credentials (resolved at runtime)
    credentials: dict[str, SecretStr] = Field(default_factory=dict)

class DuckDBComputeConfig(ComputeConfig):
    """DuckDB-specific compute configuration."""
    plugin: Literal["duckdb"] = "duckdb"

    path: str = Field(default=":memory:", description="Database path")
    memory_limit: str = Field(default="4GB", description="Max memory")
    extensions: list[str] = Field(default_factory=list, description="Extensions to load")
```

---

## 7. Error Handling for Compute Operations

### Decision
Specific exception types for compute failures with correlation IDs for debugging.

### Rationale
- Structured errors enable programmatic handling
- Correlation IDs link errors to specific operations
- Consistent with FR-022, FR-023 requirements

### Error Hierarchy
```python
class ComputeError(Exception):
    """Base exception for compute errors."""
    def __init__(self, message: str, correlation_id: str | None = None):
        super().__init__(message)
        self.correlation_id = correlation_id

class ComputeConnectionError(ComputeError):
    """Connection to compute target failed."""
    pass

class ComputeTimeoutError(ComputeError):
    """Operation timed out."""
    pass

class ComputeConfigurationError(ComputeError):
    """Invalid compute configuration."""
    pass
```

---

## 8. OpenTelemetry Metrics Integration

### Decision
Emit metrics via OpenTelemetry for observability (FR-024).

### Rationale
- OTel is enforced standard per constitution
- Metrics: connection validation latency, error rates
- Traces: span per validate_connection() call

### Metrics to Emit
| Metric | Type | Labels |
|--------|------|--------|
| `floe.compute.validation.duration_ms` | Histogram | plugin, status |
| `floe.compute.validation.errors` | Counter | plugin, error_type |
| `floe.compute.profile.generation_ms` | Histogram | plugin |

### Implementation Pattern
```python
from opentelemetry import trace, metrics

tracer = trace.get_tracer("floe.compute")
meter = metrics.get_meter("floe.compute")

validation_duration = meter.create_histogram(
    name="floe.compute.validation.duration_ms",
    description="Connection validation latency",
    unit="ms"
)

def validate_connection(self, config: ComputeConfig) -> ConnectionResult:
    with tracer.start_as_current_span("validate_connection") as span:
        span.set_attribute("compute.plugin", self.name)
        result = self._do_validation(config)
        span.set_attribute("compute.status", result.status.value)
        validation_duration.record(result.latency_ms, {"plugin": self.name})
        return result
```

---

## Summary

All research items resolved. Key decisions:

1. **dbt-duckdb Profile**: YAML structure with path, threads, extensions, settings, attach blocks
2. **Iceberg Extension**: ATTACH statements with REST catalog OAuth2 authentication
3. **K8s Resources**: ResourceSpec model with small/medium/large presets
4. **Connection Validation**: Native drivers for fast health checks (<5s)
5. **dbt Packages**: `get_required_dbt_packages()` returns adapter dependencies
6. **ComputeConfig**: Pydantic model with plugin-specific subclasses
7. **Error Handling**: Structured exceptions with correlation IDs
8. **Observability**: OpenTelemetry metrics and traces

**Next Steps**: Proceed to Phase 1 - Generate data-model.md and contracts/
