"""E2E test: Compile → Deploy → Materialize → Validate (AC-2.2).

THE critical user workflow test. Validates the full data product lifecycle:
1. Compile demo/customer-360/floe.yaml through the real 6-stage pipeline
2. Verify CompiledArtifacts contain expected products
3. Verify Dagster code locations are loaded
4. Trigger asset materialization via Dagster GraphQL
5. Validate Iceberg tables exist in Polaris catalog

This test exercises the REAL compilation pipeline feeding REAL charts.
Zero mocks, zero custom infrastructure.

Prerequisites:
    - Kind cluster running with all services: make kind-up
    - Port-forwards active: make test-e2e (manages lifecycle)

See Also:
    - .specwright/work/test-hardening-audit/spec.md: AC-2.2
    - packages/floe-core/src/floe_core/compilation/stages.py: 6-stage pipeline
    - demo/customer-360/floe.yaml: Demo product spec
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import TYPE_CHECKING, Any

import httpx
import pytest

if TYPE_CHECKING:
    from collections.abc import Callable


# Demo product paths relative to project root
DEMO_PRODUCTS = {
    "customer-360": "demo/customer-360/floe.yaml",
    "iot-telemetry": "demo/iot-telemetry/floe.yaml",
    "financial-risk": "demo/financial-risk/floe.yaml",
}


@pytest.mark.e2e
@pytest.mark.requirement("AC-2.2")
class TestCompileDeployMaterialize:
    """The critical user workflow: compile → deploy → materialize → validate.

    This is the most important E2E test in the platform. It validates that
    a data engineer can go from a floe.yaml spec to materialized Iceberg
    tables in a single workflow.
    """

    @pytest.mark.requirement("AC-2.2")
    def test_compile_all_demo_products(
        self,
        compiled_artifacts: Callable[[Path], Any],
        project_root: Path,
    ) -> None:
        """Compile all 3 demo products through the real 6-stage pipeline.

        Validates that each demo product compiles successfully and produces
        CompiledArtifacts with correct metadata, resolved plugins, and transforms.
        """
        for product_name, rel_path in DEMO_PRODUCTS.items():
            spec_path = project_root / rel_path
            assert spec_path.exists(), (
                f"Demo product spec not found: {spec_path}\n"
                f"Demo products must exist for E2E testing."
            )

            artifacts = compiled_artifacts(spec_path)

            # Verify version is valid semver
            assert artifacts.version, f"CompiledArtifacts.version is empty for {product_name}"

            # Verify metadata has correct product name
            assert artifacts.metadata.product_name == product_name, (
                f"Expected product_name={product_name}, got {artifacts.metadata.product_name}"
            )

            # Verify plugins were resolved
            assert artifacts.plugins is not None, f"Plugins not resolved for {product_name}"
            assert artifacts.plugins.compute is not None, (
                f"Compute plugin not resolved for {product_name}"
            )
            assert artifacts.plugins.compute.type == "duckdb", (
                f"Expected compute=duckdb for {product_name}, got {artifacts.plugins.compute.type}"
            )

            # Verify transforms were resolved
            assert artifacts.transforms is not None, f"Transforms not resolved for {product_name}"
            assert len(artifacts.transforms.models) > 0, (
                f"No transforms resolved for {product_name}"
            )
            # Content validation: every model must have name and compute
            for model in artifacts.transforms.models:
                assert model.name, f"Model missing name in {product_name}"
                assert model.compute, f"Model {model.name} missing compute target in {product_name}"

            # Verify observability config present
            assert artifacts.observability is not None, (
                f"Observability config missing for {product_name}"
            )

    @pytest.mark.requirement("AC-2.2")
    def test_compiled_artifacts_contain_dbt_profiles(
        self,
        compiled_artifacts: Callable[[Path], Any],
        project_root: Path,
    ) -> None:
        """Verify compilation generates dbt profiles for each product.

        The dbt_profiles field is critical — it tells dbt how to connect
        to the compute engine. Without it, dbt run will fail.
        """
        spec_path = project_root / DEMO_PRODUCTS["customer-360"]
        artifacts = compiled_artifacts(spec_path)

        assert artifacts.dbt_profiles is not None, (
            "dbt_profiles not generated by compilation.\n"
            "Stage 5 (COMPILE) should produce profiles.yml content."
        )
        assert isinstance(artifacts.dbt_profiles, dict), (
            f"dbt_profiles should be dict, got {type(artifacts.dbt_profiles)}"
        )

    @pytest.mark.requirement("AC-2.2")
    @pytest.mark.xfail(
        reason=(
            "Pipeline gap: compile_pipeline() does not pass enforcement_result "
            "to build_artifacts() — see stages.py:368"
        ),
        strict=True,
    )
    def test_compiled_artifacts_enforcement(
        self,
        compiled_artifacts: Callable[[Path], Any],
        project_root: Path,
    ) -> None:
        """Verify compilation runs the ENFORCE stage.

        The enforcement_result field indicates that governance policies
        were evaluated during compilation (Stage 4).
        """
        spec_path = project_root / DEMO_PRODUCTS["customer-360"]
        artifacts = compiled_artifacts(spec_path)

        # Enforcement should have run (even if in warn mode)
        assert artifacts.enforcement_result is not None, (
            "enforcement_result is None — Stage 4 (ENFORCE) did not produce results.\n"
            "Governance enforcement must run during compilation."
        )
        # Demo uses warn mode — should pass
        assert artifacts.enforcement_result.passed, (
            f"Enforcement failed for customer-360 in warn mode.\n"
            f"Errors: {artifacts.enforcement_result.error_count}\n"
            f"Enforcement level: {artifacts.enforcement_result.enforcement_level}"
        )

    @pytest.mark.requirement("AC-2.2")
    def test_dagster_code_locations_loaded(
        self,
        wait_for_service: Callable[..., None],
    ) -> None:
        """Verify Dagster has loaded the demo code locations.

        The floe-jobs chart configures Dagster workspace with code locations
        for each demo product. This test queries the Dagster GraphQL API
        to verify the locations are actually loaded and responsive.
        """
        dagster_url = os.environ.get("DAGSTER_URL", "http://localhost:3000")
        wait_for_service(
            f"{dagster_url}/server_info",
            timeout=60,
            description="Dagster webserver",
        )

        # Query for repositories via GraphQL (current Dagster API)
        query = """
        {
            repositoriesOrError {
                ... on RepositoryConnection {
                    nodes {
                        name
                        location {
                            name
                        }
                    }
                }
                ... on PythonError {
                    message
                }
            }
        }
        """

        response = httpx.post(
            f"{dagster_url}/graphql",
            json={"query": query},
            timeout=30.0,
        )
        assert response.status_code == 200, (
            f"Dagster GraphQL returned {response.status_code}: {response.text}"
        )

        data = response.json()
        assert "data" in data, f"GraphQL error: {data}"

        repos_data = data["data"]["repositoriesOrError"]

        # Check it's not a Python error
        if "message" in repos_data:
            pytest.fail(f"Dagster repository error: {repos_data['message']}")

        nodes = repos_data.get("nodes", [])
        # Verify at least one repository is loaded
        assert len(nodes) > 0, (
            "No Dagster repositories found.\n"
            "The floe-jobs chart should configure workspace with demo products.\n"
            "Check: helm status floe-platform -n floe-test"
        )

        # Verify repositories have location info
        for node in nodes:
            assert "name" in node, f"Repository node missing 'name': {node}"
            assert "location" in node, f"Repository '{node['name']}' missing 'location' info"

    @pytest.mark.requirement("AC-2.2")
    def test_dagster_assets_visible(
        self,
        wait_for_service: Callable[..., None],
    ) -> None:
        """Verify Dagster exposes assets from compiled products.

        After code locations are loaded, Dagster should expose the assets
        defined in the demo products (dbt models become Dagster assets).
        """
        dagster_url = os.environ.get("DAGSTER_URL", "http://localhost:3000")
        wait_for_service(
            f"{dagster_url}/server_info",
            timeout=60,
            description="Dagster webserver",
        )

        # Query for asset nodes
        query = """
        {
            assetNodes {
                id
                assetKey {
                    path
                }
                repository {
                    name
                    location {
                        name
                    }
                }
            }
        }
        """

        response = httpx.post(
            f"{dagster_url}/graphql",
            json={"query": query},
            timeout=30.0,
        )
        assert response.status_code == 200, f"Dagster GraphQL returned {response.status_code}"

        data = response.json()
        assert "data" in data, f"GraphQL error: {data}"

        assets = data["data"].get("assetNodes", [])
        assert len(assets) > 0, (
            "No Dagster assets found. Code locations should expose dbt models as assets."
        )
        asset_keys = ["/".join(a["assetKey"]["path"]) for a in assets]
        customer_assets = [k for k in asset_keys if "customer" in k.lower()]
        assert len(customer_assets) > 0, (
            f"No customer-360 assets found among {len(assets)} assets. "
            f"Sample keys: {asset_keys[:10]}"
        )

    @pytest.mark.requirement("AC-2.2")
    def test_polaris_catalog_accessible(
        self,
        polaris_client: Any,
    ) -> None:
        """Verify Polaris catalog is accessible and can list namespaces.

        The Polaris REST catalog must be functional for Iceberg table
        operations after compilation.
        """
        # List namespaces to verify catalog connectivity
        try:
            namespaces = polaris_client.list_namespaces()
            # Even if no namespaces exist yet, the call should succeed
            assert isinstance(namespaces, list), (
                f"list_namespaces should return list, got {type(namespaces)}"
            )
        except Exception as e:
            pytest.fail(
                f"Polaris catalog not accessible: {e}\n"
                "Verify Polaris is running and port-forward is active."
            )

    @pytest.mark.requirement("AC-2.2")
    def test_compilation_produces_serializable_artifacts(
        self,
        compiled_artifacts: Callable[[Path], Any],
        project_root: Path,
        tmp_path: Path,
    ) -> None:
        """Verify CompiledArtifacts can be serialized to JSON and deserialized.

        The compilation output must be serializable because it's consumed by
        downstream tools (Helm chart values, Dagster code locations, etc.).
        """
        import json

        spec_path = project_root / DEMO_PRODUCTS["customer-360"]
        artifacts = compiled_artifacts(spec_path)

        # Serialize to JSON
        json_str = artifacts.model_dump_json()
        assert len(json_str) > 0, "Serialized artifacts is empty"

        # Verify it's valid JSON
        parsed = json.loads(json_str)
        assert isinstance(parsed, dict), "Serialized artifacts is not a dict"
        assert "version" in parsed, "Serialized artifacts missing 'version'"
        assert "metadata" in parsed, "Serialized artifacts missing 'metadata'"

        # Write to file and read back
        output_path = tmp_path / "compiled_artifacts.json"
        output_path.write_text(json_str)
        assert output_path.exists(), "Failed to write artifacts to file"

        # Verify round-trip
        from floe_core.schemas.compiled_artifacts import CompiledArtifacts

        restored = CompiledArtifacts.model_validate_json(output_path.read_text())
        assert restored.version == artifacts.version, (
            f"Round-trip version mismatch: {restored.version} != {artifacts.version}"
        )
        assert restored.metadata.product_name == artifacts.metadata.product_name, (
            "Round-trip product_name mismatch"
        )

    @pytest.mark.requirement("AC-2.2")
    @pytest.mark.xfail(
        reason="Requires code location mounting in Kind — validates intent",
        strict=False,
    )
    def test_trigger_asset_materialization(
        self,
        wait_for_service: Callable[..., None],
    ) -> None:
        """Trigger asset materialization via Dagster GraphQL API.

        Uses launchRun mutation to materialize a single asset (stg_customers)
        and polls for run completion. Validates the deploy → materialize step
        of the full lifecycle.
        """
        from testing.fixtures.polling import wait_for_condition

        dagster_url = os.environ.get("DAGSTER_URL", "http://localhost:3000")
        wait_for_service(
            f"{dagster_url}/server_info",
            timeout=60,
            description="Dagster webserver",
        )

        # Launch materialization for stg_customers asset
        mutation = """
        mutation LaunchRun($executionParams: ExecutionParams!) {
            launchRun(executionParams: $executionParams) {
                ... on LaunchRunSuccess {
                    run {
                        runId
                        status
                    }
                }
                ... on PythonError {
                    message
                }
                ... on RunConfigValidationInvalid {
                    errors { message }
                }
            }
        }
        """

        variables = {
            "executionParams": {
                "selector": {
                    "assetSelection": [{"path": ["stg_customers"]}],
                },
                "mode": "default",
            },
        }

        response = httpx.post(
            f"{dagster_url}/graphql",
            json={"query": mutation, "variables": variables},
            timeout=30.0,
        )
        assert response.status_code == 200, (
            f"Dagster GraphQL returned {response.status_code}: {response.text}"
        )

        data = response.json()
        launch_result = data.get("data", {}).get("launchRun", {})

        # Check for launch error
        if "message" in launch_result:
            pytest.fail(f"Materialization launch failed: {launch_result['message']}")
        if "errors" in launch_result:
            errors = [e["message"] for e in launch_result["errors"]]
            pytest.fail(f"Run config invalid: {errors}")

        run_id = launch_result["run"]["runId"]
        assert run_id, "No runId returned from launchRun"

        # Poll for run completion
        def check_run_complete() -> bool:
            """Check if materialization run has completed."""
            status_query = """
            query RunStatus($runId: ID!) {
                runOrError(runId: $runId) {
                    ... on Run {
                        status
                    }
                }
            }
            """
            resp = httpx.post(
                f"{dagster_url}/graphql",
                json={"query": status_query, "variables": {"runId": run_id}},
                timeout=10.0,
            )
            if resp.status_code != 200:
                return False
            status = resp.json().get("data", {}).get("runOrError", {}).get("status")
            return status in ("SUCCESS", "FAILURE", "CANCELED")

        completed = wait_for_condition(
            check_run_complete,
            timeout=120.0,
            interval=5.0,
            description="materialization run to complete",
            raise_on_timeout=False,
        )
        assert completed, f"Materialization run {run_id} did not complete within 120s"

        # Verify final status is SUCCESS
        status_query = """
        query RunStatus($runId: ID!) {
            runOrError(runId: $runId) {
                ... on Run { status }
            }
        }
        """
        final_resp = httpx.post(
            f"{dagster_url}/graphql",
            json={"query": status_query, "variables": {"runId": run_id}},
            timeout=10.0,
        )
        final_status = final_resp.json().get("data", {}).get("runOrError", {}).get("status")
        assert final_status == "SUCCESS", (
            f"Materialization run {run_id} ended with status {final_status}"
        )

    @pytest.mark.requirement("AC-2.2")
    @pytest.mark.xfail(
        reason="Requires code location mounting in Kind — validates intent",
        strict=False,
    )
    def test_iceberg_tables_exist_after_materialization(
        self,
        polaris_client: Any,
    ) -> None:
        """Verify Iceberg tables exist in Polaris after materialization.

        After asset materialization, the product namespace should contain
        at least one Iceberg table with expected schema columns. Validates
        the materialize → validate step of the full lifecycle.
        """
        # List tables in the customer-360 namespace
        # The namespace may be "customer_360" or "customer-360" depending on
        # how Dagster normalizes the product name
        namespaces = polaris_client.list_namespaces()
        customer_namespaces = [ns for ns in namespaces if "customer" in str(ns).lower()]
        assert len(customer_namespaces) > 0, (
            f"No customer namespace found in Polaris catalog. Available namespaces: {namespaces}"
        )

        # Check tables in the first matching namespace
        target_ns = customer_namespaces[0]
        tables = polaris_client.list_tables(target_ns)
        assert len(tables) > 0, (
            f"No Iceberg tables found in namespace {target_ns}. "
            f"Materialization may not have written any tables."
        )

        # Verify at least one table has schema columns
        first_table = polaris_client.load_table(tables[0])
        schema = first_table.schema()
        assert len(schema.fields) > 0, f"Table {tables[0]} has empty schema — no columns defined"
